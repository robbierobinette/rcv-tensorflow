{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from Tensor import Tensor\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ActorModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__(self)\n",
    "\n",
    "        #self.l1 = tf.keras.layers.Dense(32)\n",
    "        self.actions = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state: Tensor, training: bool = None, mask: bool = None) -> Tensor:\n",
    "        #x = self.l1(state)\n",
    "        actions = self.actions(state)\n",
    "        return actions\n",
    "\n",
    "class CriticModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__(self)\n",
    "        self.l1 = tf.keras.layers.Dense(2)\n",
    "        self.value = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state: Tensor, training: bool = None, mask: bool = None) -> Tensor:\n",
    "        c = self.l1(state)\n",
    "        value = self.value(c)\n",
    "        return value\n",
    "\n",
    "class ZModel():\n",
    "    def __init__(self, input_dim: int):\n",
    "        self.actor = ActorModel(1)\n",
    "        self.critic = CriticModel(1)\n",
    "        self.cadam = tf.optimizers.Adam()\n",
    "        self.aadam = tf.optimizers.Adam()\n",
    "\n",
    "    def update(self, state: Tensor, reward: Tensor):\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            action = self.actor(state, training=True, mask=None)\n",
    "\n",
    "        with tf.GradientTape() as c_tape1, tf.GradientTape() as c_tape2:\n",
    "            action_var = tf.Variable(action)\n",
    "            ci = tf.concat([action_var, state], axis=1)\n",
    "            critic_value = self.critic.call(ci)\n",
    "            critic_loss = tf.reduce_mean(tf.square(critic_value - reward))\n",
    "\n",
    "        # critic_grads = c_tape1.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_grads = c_tape1.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_grad_vars = zip(critic_grads, self.critic.trainable_variables)\n",
    "        print(\"critic_grads--------------------------------------------------------------------------------:\")\n",
    "        for g in critic_grads:\n",
    "            print(\"g\")\n",
    "            print( g)\n",
    "\n",
    "        critic_dvda = c_tape2.gradient(critic_value, action_var)\n",
    "        print(\"critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "        print(critic_dvda)\n",
    "        print(tf.concat([state, action, reward, critic_dvda], axis=1))\n",
    "        print(\"critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "        print(critic_dvda.shape)\n",
    "        for i in range(state.shape[0]):\n",
    "            print(\"d:  state: % 7.3f  action: % 7.3f  reward: % 7.3f critic_dvda % 7.3f \" %\n",
    "                  ( state[i,0],\n",
    "                    action[i,0],\n",
    "                    reward[i,0],\n",
    "                    critic_dvda[i,0])\n",
    "                  )\n",
    "\n",
    "        actor_grads = actor_tape.gradient(action, self.actor.trainable_variables, -critic_dvda)\n",
    "        actor_grad_vars = zip(actor_grads, self.actor.trainable_variables)\n",
    "\n",
    "        self.cadam.apply_gradients(critic_grad_vars)\n",
    "        self.aadam.apply_gradients(actor_grad_vars)\n",
    "\n",
    "        return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.7785758 -0.8087036]\n",
      " [-2.3961384 -2.4888597]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-1.5490271 -1.6089684], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.28351495]\n",
      " [1.3802123 ]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-1.4480264], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]\n",
      " [-1.967444]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.0764124  -0.349758    0.65443885 -1.967444  ]\n",
      " [ 1.0589856   0.3440955   0.66178364 -1.967444  ]\n",
      " [ 0.689916    0.22417396  0.8217497  -1.967444  ]\n",
      " [-0.88178575 -0.28651807  0.73836505 -1.967444  ]\n",
      " [-0.15901344 -0.05166813  0.98860824 -1.967444  ]\n",
      " [ 0.46051723  0.14963557  0.9118701  -1.967444  ]\n",
      " [ 0.14039709  0.04561913  0.9910971  -1.967444  ]\n",
      " [ 1.1156322   0.36250165  0.63807833 -1.967444  ]\n",
      " [-1.7153468  -0.55736655  0.42718282 -1.967444  ]\n",
      " [ 2.1635954   0.7030157   0.31915304 -1.967444  ]\n",
      " [-1.1707294  -0.38040435  0.6155309  -1.967444  ]\n",
      " [-2.0291908  -0.65934366  0.347647   -1.967444  ]\n",
      " [ 0.5990262   0.1946412   0.8594556  -1.967444  ]\n",
      " [ 0.9860482   0.320396    0.6929561  -1.967444  ]\n",
      " [ 0.9321467   0.30288184  0.71634555 -1.967444  ]\n",
      " [-0.6652932  -0.21617329  0.83214843 -1.967444  ]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -1.076  action:  -0.350  reward:   0.654 critic_dvda  -1.967 \n",
      "d:  state:   1.059  action:   0.344  reward:   0.662 critic_dvda  -1.967 \n",
      "d:  state:   0.690  action:   0.224  reward:   0.822 critic_dvda  -1.967 \n",
      "d:  state:  -0.882  action:  -0.287  reward:   0.738 critic_dvda  -1.967 \n",
      "d:  state:  -0.159  action:  -0.052  reward:   0.989 critic_dvda  -1.967 \n",
      "d:  state:   0.461  action:   0.150  reward:   0.912 critic_dvda  -1.967 \n",
      "d:  state:   0.140  action:   0.046  reward:   0.991 critic_dvda  -1.967 \n",
      "d:  state:   1.116  action:   0.363  reward:   0.638 critic_dvda  -1.967 \n",
      "d:  state:  -1.715  action:  -0.557  reward:   0.427 critic_dvda  -1.967 \n",
      "d:  state:   2.164  action:   0.703  reward:   0.319 critic_dvda  -1.967 \n",
      "d:  state:  -1.171  action:  -0.380  reward:   0.616 critic_dvda  -1.967 \n",
      "d:  state:  -2.029  action:  -0.659  reward:   0.348 critic_dvda  -1.967 \n",
      "d:  state:   0.599  action:   0.195  reward:   0.859 critic_dvda  -1.967 \n",
      "d:  state:   0.986  action:   0.320  reward:   0.693 critic_dvda  -1.967 \n",
      "d:  state:   0.932  action:   0.303  reward:   0.716 critic_dvda  -1.967 \n",
      "d:  state:  -0.665  action:  -0.216  reward:   0.832 critic_dvda  -1.967 \n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-1.1774249 -1.2230294]\n",
      " [-3.6395197 -3.7804866]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-1.5224894 -1.5814588], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.41946208]\n",
      " [2.0893297 ]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-1.424551], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]\n",
      " [-1.9634509]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.6011958  -0.19574498  0.85881853 -1.9634509 ]\n",
      " [ 0.88790476  0.28661844  0.73445964 -1.9634509 ]\n",
      " [ 1.7096094   0.5527927   0.42767486 -1.9634509 ]\n",
      " [-0.40505886 -0.13221046  0.93071204 -1.9634509 ]\n",
      " [ 0.827254    0.2669719   0.7610836  -1.9634509 ]\n",
      " [-0.39738938 -0.1297261   0.93314606 -1.9634509 ]\n",
      " [ 1.1219457   0.3624312   0.6341708  -1.9634509 ]\n",
      " [ 2.965354    0.9595653   0.19907674 -1.9634509 ]\n",
      " [-2.0341775  -0.6599298   0.3461936  -1.9634509 ]\n",
      " [-1.3930086  -0.45223638  0.53048927 -1.9634509 ]\n",
      " [ 0.67245156  0.21682681  0.82809293 -1.9634509 ]\n",
      " [-0.68799704 -0.22386245  0.8227602  -1.9634509 ]\n",
      " [-1.33855    -0.43459564  0.55031735 -1.9634509 ]\n",
      " [ 0.13257852  0.04194608  0.9918527  -1.9634509 ]\n",
      " [ 2.2515266   0.7283356   0.30119476 -1.9634509 ]\n",
      " [-2.1152415  -0.6861888   0.32871038 -1.9634509 ]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.601  action:  -0.196  reward:   0.859 critic_dvda  -1.963 \n",
      "d:  state:   0.888  action:   0.287  reward:   0.734 critic_dvda  -1.963 \n",
      "d:  state:   1.710  action:   0.553  reward:   0.428 critic_dvda  -1.963 \n",
      "d:  state:  -0.405  action:  -0.132  reward:   0.931 critic_dvda  -1.963 \n",
      "d:  state:   0.827  action:   0.267  reward:   0.761 critic_dvda  -1.963 \n",
      "d:  state:  -0.397  action:  -0.130  reward:   0.933 critic_dvda  -1.963 \n",
      "d:  state:   1.122  action:   0.362  reward:   0.634 critic_dvda  -1.963 \n",
      "d:  state:   2.965  action:   0.960  reward:   0.199 critic_dvda  -1.963 \n",
      "d:  state:  -2.034  action:  -0.660  reward:   0.346 critic_dvda  -1.963 \n",
      "d:  state:  -1.393  action:  -0.452  reward:   0.530 critic_dvda  -1.963 \n",
      "d:  state:   0.672  action:   0.217  reward:   0.828 critic_dvda  -1.963 \n",
      "d:  state:  -0.688  action:  -0.224  reward:   0.823 critic_dvda  -1.963 \n",
      "d:  state:  -1.339  action:  -0.435  reward:   0.550 critic_dvda  -1.963 \n",
      "d:  state:   0.133  action:   0.042  reward:   0.992 critic_dvda  -1.963 \n",
      "d:  state:   2.252  action:   0.728  reward:   0.301 critic_dvda  -1.963 \n",
      "d:  state:  -2.115  action:  -0.686  reward:   0.329 critic_dvda  -1.963 \n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.7851125 -0.815551 ]\n",
      " [-2.43547   -2.5298927]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.810638  -0.8420662], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.2743544]\n",
      " [1.3941883]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.7591959], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]\n",
      " [-1.9594989]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-2.1819346  -0.7068336   0.3148691  -1.9594989 ]\n",
      " [ 1.1018333   0.353927    0.64128715 -1.9594989 ]\n",
      " [-0.560961   -0.18320805  0.87512237 -1.9594989 ]\n",
      " [ 0.20189928  0.06321989  0.98113096 -1.9594989 ]\n",
      " [ 1.4315777   0.4604448   0.5146418  -1.9594989 ]\n",
      " [-0.24652827 -0.08163635  0.9735303  -1.9594989 ]\n",
      " [-1.1320206  -0.3676783   0.63122576 -1.9594989 ]\n",
      " [ 0.9226096   0.29603207  0.71808153 -1.9594989 ]\n",
      " [ 0.9233494   0.29627103  0.717758   -1.9594989 ]\n",
      " [-1.248961   -0.40545374  0.58428127 -1.9594989 ]\n",
      " [-2.6772234  -0.86682755  0.23377958 -1.9594989 ]\n",
      " [-1.9706105  -0.6385693   0.36044687 -1.9594989 ]\n",
      " [ 0.3131412   0.09915454  0.9562146  -1.9594989 ]\n",
      " [-0.65138245 -0.21241702  0.8384404  -1.9594989 ]\n",
      " [ 0.03791467  0.01024769  0.99923515 -1.9594989 ]\n",
      " [-0.4602193  -0.1506653   0.9125556  -1.9594989 ]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -2.182  action:  -0.707  reward:   0.315 critic_dvda  -1.959 \n",
      "d:  state:   1.102  action:   0.354  reward:   0.641 critic_dvda  -1.959 \n",
      "d:  state:  -0.561  action:  -0.183  reward:   0.875 critic_dvda  -1.959 \n",
      "d:  state:   0.202  action:   0.063  reward:   0.981 critic_dvda  -1.959 \n",
      "d:  state:   1.432  action:   0.460  reward:   0.515 critic_dvda  -1.959 \n",
      "d:  state:  -0.247  action:  -0.082  reward:   0.974 critic_dvda  -1.959 \n",
      "d:  state:  -1.132  action:  -0.368  reward:   0.631 critic_dvda  -1.959 \n",
      "d:  state:   0.923  action:   0.296  reward:   0.718 critic_dvda  -1.959 \n",
      "d:  state:   0.923  action:   0.296  reward:   0.718 critic_dvda  -1.959 \n",
      "d:  state:  -1.249  action:  -0.405  reward:   0.584 critic_dvda  -1.959 \n",
      "d:  state:  -2.677  action:  -0.867  reward:   0.234 critic_dvda  -1.959 \n",
      "d:  state:  -1.971  action:  -0.639  reward:   0.360 critic_dvda  -1.959 \n",
      "d:  state:   0.313  action:   0.099  reward:   0.956 critic_dvda  -1.959 \n",
      "d:  state:  -0.651  action:  -0.212  reward:   0.838 critic_dvda  -1.959 \n",
      "d:  state:   0.038  action:   0.010  reward:   0.999 critic_dvda  -1.959 \n",
      "d:  state:  -0.460  action:  -0.151  reward:   0.913 critic_dvda  -1.959 \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def r(net_out: float, state: float) -> float:\n",
    "    delta = net_out - state\n",
    "    probability = 1 / (1 + delta * delta)\n",
    "    return probability\n",
    "    # if random.uniform(0, 1) < probability:\n",
    "    #     return 1\n",
    "    # else:\n",
    "    #     return 0\n",
    "\n",
    "def create_block(model: ZModel):\n",
    "    block_size = 16\n",
    "    state = tf.random.normal(shape=(block_size, 1))\n",
    "    actions= model.actor(state)\n",
    "    rewards = np.array([r(state[i,0],actions[i, 0] ) for i in range(block_size)]).reshape((block_size,1))\n",
    "    return state, actions, rewards\n",
    "\n",
    "\n",
    "class RingSum:\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 10\n",
    "        self.values = []\n",
    "        self.index = 0\n",
    "    def add(self, x):\n",
    "        if len(self.values) < self.max:\n",
    "            self.values.append(x)\n",
    "            self.sum += x\n",
    "        else:\n",
    "            xx = self.values[self.index]\n",
    "            self.values[self.index] = x\n",
    "            self.sum -= xx\n",
    "            self.sum += x\n",
    "            self.index = (self.index + 1) % self.max\n",
    "        return self.sum\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / len(self.values)\n",
    "\n",
    "def train():\n",
    "    model = ZModel(1)\n",
    "    rewards = RingSum()\n",
    "    losses = RingSum()\n",
    "    for i in range(3):\n",
    "        state, action, reward = create_block(model)\n",
    "        loss = model.update(state, reward)\n",
    "        rewards.add(np.mean(reward))\n",
    "        losses.add(np.mean(loss))\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            print(\"%4d %.2f %.2f\" % (i, rewards.mean(), losses.mean()))\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}