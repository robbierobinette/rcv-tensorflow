{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from Tensor import Tensor\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class ActorModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__(self)\n",
    "\n",
    "        #self.l1 = tf.keras.layers.Dense(32)\n",
    "        self.actions = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state: Tensor, training: bool = None, mask: bool = None) -> Tensor:\n",
    "        #x = self.l1(state)\n",
    "        actions = self.actions(state)\n",
    "        return actions\n",
    "\n",
    "class CriticModel(tf.keras.Model):\n",
    "    def __init__(self, input_dim: int):\n",
    "        super().__init__(self)\n",
    "        self.l1 = tf.keras.layers.Dense(2)\n",
    "        self.value = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, state: Tensor, training: bool = None, mask: bool = None) -> Tensor:\n",
    "        c = self.l1(state)\n",
    "        value = self.value(c)\n",
    "        return value\n",
    "\n",
    "class ZModel():\n",
    "    def __init__(self, input_dim: int):\n",
    "        self.actor = ActorModel(1)\n",
    "        self.critic = CriticModel(1)\n",
    "        self.cadam = tf.optimizers.Adam()\n",
    "        self.aadam = tf.optimizers.Adam()\n",
    "\n",
    "        self.global_step = 0\n",
    "\n",
    "    def update(self, state: Tensor, reward: Tensor):\n",
    "        with tf.GradientTape() as actor_tape:\n",
    "            action = self.actor(state, training=True, mask=None)\n",
    "\n",
    "        with tf.GradientTape() as c_tape1, tf.GradientTape() as c_tape2:\n",
    "            action_var = tf.Variable(action)\n",
    "            ci = tf.concat([action_var, state], axis=1)\n",
    "            critic_value = self.critic.call(ci)\n",
    "            critic_loss = tf.reduce_mean(tf.square(critic_value - reward))\n",
    "\n",
    "        # critic_grads = c_tape1.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_grads = c_tape1.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        critic_grad_vars = zip(critic_grads, self.critic.trainable_variables)\n",
    "\n",
    "        critic_dvda = c_tape2.gradient(critic_value, action_var)\n",
    "        if self.global_step % 100 == 0:\n",
    "            print(\"global_step \", self.global_step)\n",
    "            print(\"critic_grads--------------------------------------------------------------------------------:\")\n",
    "            for g in critic_grads:\n",
    "                print(\"g\")\n",
    "                print( g)\n",
    "\n",
    "            print(\"critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "            print(critic_dvda)\n",
    "            print(tf.concat([state, action, reward, critic_dvda], axis=1))\n",
    "            print(\"critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\")\n",
    "            print(critic_dvda.shape)\n",
    "            cl = tf.square(critic_value - reward)\n",
    "            for i in range(state.shape[0]):\n",
    "                print(\"d:  state: % 7.3f  action: % 7.3f  reward: % 7.3f c_value % 7.3f critic_dvda % 7.3f cl % 7.3f\" %\n",
    "                      ( state[i,0],\n",
    "                        action[i,0],\n",
    "                        reward[i,0],\n",
    "                        critic_value[i, 0],\n",
    "                        critic_dvda[i,0],\n",
    "                        cl[i,0],\n",
    "                        )\n",
    "                      )\n",
    "\n",
    "        actor_grads = actor_tape.gradient(action, self.actor.trainable_variables, -critic_dvda)\n",
    "        actor_grad_vars = zip(actor_grads, self.actor.trainable_variables)\n",
    "\n",
    "        self.cadam.apply_gradients(critic_grad_vars)\n",
    "        self.aadam.apply_gradients(actor_grad_vars)\n",
    "        self.global_step += 1\n",
    "\n",
    "        return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step  0\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.0568437   0.0948899 ]\n",
      " [-0.41267583  0.688885  ]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.6791256  1.1336731], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.05869703]\n",
      " [-0.44498602]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-1.5800308], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]\n",
      " [-0.15976788]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.5054363  -0.06962091  0.840382   -0.15976788]\n",
      " [ 0.9212      0.12688994  0.6131477  -0.15976788]\n",
      " [ 0.7651532   0.10539541  0.696728   -0.15976788]\n",
      " [ 0.8055      0.11095294  0.67458373 -0.15976788]\n",
      " [-0.30592674 -0.04213963  0.93494326 -0.15976788]\n",
      " [ 2.2378557   0.30825162  0.21171318 -0.15976788]\n",
      " [ 1.8119419   0.24958447  0.2906165  -0.15976788]\n",
      " [-1.1540155  -0.15895893  0.50247777 -0.15976788]\n",
      " [ 0.10948677  0.01508117  0.9911663  -0.15976788]\n",
      " [-1.2990832  -0.17894116  0.44351438 -0.15976788]\n",
      " [ 1.013869    0.13965456  0.56681275 -0.15976788]\n",
      " [-0.6315786  -0.08699629  0.77126575 -0.15976788]\n",
      " [ 1.160808    0.15989456  0.4995435  -0.15976788]\n",
      " [-0.25775713 -0.03550455  0.95292896 -0.15976788]\n",
      " [ 0.51209205  0.0705377   0.8368409  -0.15976788]\n",
      " [-0.10647777 -0.01466669  0.99164116 -0.15976788]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.505  action:  -0.070  reward:   0.840 c_value   0.181 critic_dvda  -0.160 cl   0.434\n",
      "d:  state:   0.921  action:   0.127  reward:   0.613 c_value  -0.331 critic_dvda  -0.160 cl   0.891\n",
      "d:  state:   0.765  action:   0.105  reward:   0.697 c_value  -0.275 critic_dvda  -0.160 cl   0.943\n",
      "d:  state:   0.805  action:   0.111  reward:   0.675 c_value  -0.289 critic_dvda  -0.160 cl   0.929\n",
      "d:  state:  -0.306  action:  -0.042  reward:   0.935 c_value   0.110 critic_dvda  -0.160 cl   0.681\n",
      "d:  state:   2.238  action:   0.308  reward:   0.212 c_value  -0.803 critic_dvda  -0.160 cl   1.030\n",
      "d:  state:   1.812  action:   0.250  reward:   0.291 c_value  -0.650 critic_dvda  -0.160 cl   0.885\n",
      "d:  state:  -1.154  action:  -0.159  reward:   0.502 c_value   0.414 critic_dvda  -0.160 cl   0.008\n",
      "d:  state:   0.109  action:   0.015  reward:   0.991 c_value  -0.039 critic_dvda  -0.160 cl   1.062\n",
      "d:  state:  -1.299  action:  -0.179  reward:   0.444 c_value   0.466 critic_dvda  -0.160 cl   0.001\n",
      "d:  state:   1.014  action:   0.140  reward:   0.567 c_value  -0.364 critic_dvda  -0.160 cl   0.866\n",
      "d:  state:  -0.632  action:  -0.087  reward:   0.771 c_value   0.227 critic_dvda  -0.160 cl   0.297\n",
      "d:  state:   1.161  action:   0.160  reward:   0.500 c_value  -0.417 critic_dvda  -0.160 cl   0.839\n",
      "d:  state:  -0.258  action:  -0.036  reward:   0.953 c_value   0.092 critic_dvda  -0.160 cl   0.740\n",
      "d:  state:   0.512  action:   0.071  reward:   0.837 c_value  -0.184 critic_dvda  -0.160 cl   1.042\n",
      "d:  state:  -0.106  action:  -0.015  reward:   0.992 c_value   0.038 critic_dvda  -0.160 cl   0.909\n",
      "global_step  100\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.03603827 -0.04508122]\n",
      " [-0.15273502  0.19106027]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.5617182  0.7026681], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.20319432]\n",
      " [-0.05074489]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-1.0899175], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]\n",
      " [-0.20686756]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.29089147 -0.14490679  0.9791332  -0.20686756]\n",
      " [-1.2817484  -0.28707367  0.5026697  -0.20686756]\n",
      " [ 0.11053818 -0.08731018  0.9623306  -0.20686756]\n",
      " [-1.1994085  -0.27525967  0.53935945 -0.20686756]\n",
      " [ 3.3442097   0.37665287  0.10197429 -0.20686756]\n",
      " [-0.6745838  -0.19995847  0.8161472  -0.20686756]\n",
      " [ 1.0200667   0.0431878   0.51169413 -0.20686756]\n",
      " [-0.33757877 -0.15160543  0.96657014 -0.20686756]\n",
      " [-0.16506076 -0.12685277  0.9985423  -0.20686756]\n",
      " [-0.6291855  -0.19344479  0.84042794 -0.20686756]\n",
      " [-0.1633786  -0.12661141  0.99865    -0.20686756]\n",
      " [ 1.8165102   0.15746048  0.26649284 -0.20686756]\n",
      " [-0.12467533 -0.12105832  0.9999869  -0.20686756]\n",
      " [-0.05388319 -0.11090115  0.9967595  -0.20686756]\n",
      " [ 0.8417642   0.01760519  0.59550816 -0.20686756]\n",
      " [-0.6716267  -0.19953421  0.81774753 -0.20686756]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.291  action:  -0.145  reward:   0.979 c_value   0.287 critic_dvda  -0.207 cl   0.479\n",
      "d:  state:  -1.282  action:  -0.287  reward:   0.503 c_value   0.514 critic_dvda  -0.207 cl   0.000\n",
      "d:  state:   0.111  action:  -0.087  reward:   0.962 c_value   0.195 critic_dvda  -0.207 cl   0.589\n",
      "d:  state:  -1.199  action:  -0.275  reward:   0.539 c_value   0.496 critic_dvda  -0.207 cl   0.002\n",
      "d:  state:   3.344  action:   0.377  reward:   0.102 c_value  -0.546 critic_dvda  -0.207 cl   0.420\n",
      "d:  state:  -0.675  action:  -0.200  reward:   0.816 c_value   0.375 critic_dvda  -0.207 cl   0.194\n",
      "d:  state:   1.020  action:   0.043  reward:   0.512 c_value  -0.013 critic_dvda  -0.207 cl   0.276\n",
      "d:  state:  -0.338  action:  -0.152  reward:   0.967 c_value   0.298 critic_dvda  -0.207 cl   0.447\n",
      "d:  state:  -0.165  action:  -0.127  reward:   0.999 c_value   0.258 critic_dvda  -0.207 cl   0.548\n",
      "d:  state:  -0.629  action:  -0.193  reward:   0.840 c_value   0.365 critic_dvda  -0.207 cl   0.226\n",
      "d:  state:  -0.163  action:  -0.127  reward:   0.999 c_value   0.258 critic_dvda  -0.207 cl   0.549\n",
      "d:  state:   1.817  action:   0.157  reward:   0.266 c_value  -0.196 critic_dvda  -0.207 cl   0.214\n",
      "d:  state:  -0.125  action:  -0.121  reward:   1.000 c_value   0.249 critic_dvda  -0.207 cl   0.564\n",
      "d:  state:  -0.054  action:  -0.111  reward:   0.997 c_value   0.233 critic_dvda  -0.207 cl   0.584\n",
      "d:  state:   0.842  action:   0.018  reward:   0.596 c_value   0.027 critic_dvda  -0.207 cl   0.323\n",
      "d:  state:  -0.672  action:  -0.200  reward:   0.818 c_value   0.375 critic_dvda  -0.207 cl   0.196\n",
      " 100 0.68 0.35\n",
      "global_step  200\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.02075189  0.01972307]\n",
      " [-0.24835646  0.23604372]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.07402477  0.07035485], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.06739168]\n",
      " [-0.11835375]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.11323731], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]\n",
      " [-0.46086943]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.5319976  -0.4923525   0.48057005 -0.46086943]\n",
      " [-0.57869434 -0.34162587  0.9467891  -0.46086943]\n",
      " [-0.4486114  -0.32105845  0.98399067 -0.46086943]\n",
      " [ 0.736261   -0.13371849  0.5691959  -0.46086943]\n",
      " [ 1.4664435  -0.01826946  0.31207356 -0.46086943]\n",
      " [ 1.2239152  -0.05661558  0.37882245 -0.46086943]\n",
      " [-1.008498   -0.40958202  0.7359975  -0.46086943]\n",
      " [ 0.53328055 -0.1658117   0.6717135  -0.46086943]\n",
      " [-1.7117728  -0.5207767   0.41348425 -0.46086943]\n",
      " [-1.1808773  -0.4368369   0.64366794 -0.46086943]\n",
      " [ 1.4059346  -0.02783652  0.3272573  -0.46086943]\n",
      " [ 1.4284858  -0.02427097  0.32149148 -0.46086943]\n",
      " [-2.30207    -0.61410844  0.2597933  -0.46086943]\n",
      " [-0.9957246  -0.40756243  0.7429781  -0.46086943]\n",
      " [ 1.3299048  -0.03985758  0.34767503 -0.46086943]\n",
      " [ 0.23424968 -0.21309142  0.83325416 -0.46086943]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -1.532  action:  -0.492  reward:   0.481 c_value   0.757 critic_dvda  -0.461 cl   0.077\n",
      "d:  state:  -0.579  action:  -0.342  reward:   0.947 c_value   0.590 critic_dvda  -0.461 cl   0.127\n",
      "d:  state:  -0.449  action:  -0.321  reward:   0.984 c_value   0.567 critic_dvda  -0.461 cl   0.174\n",
      "d:  state:   0.736  action:  -0.134  reward:   0.569 c_value   0.360 critic_dvda  -0.461 cl   0.044\n",
      "d:  state:   1.466  action:  -0.018  reward:   0.312 c_value   0.231 critic_dvda  -0.461 cl   0.006\n",
      "d:  state:   1.224  action:  -0.057  reward:   0.379 c_value   0.274 critic_dvda  -0.461 cl   0.011\n",
      "d:  state:  -1.008  action:  -0.410  reward:   0.736 c_value   0.665 critic_dvda  -0.461 cl   0.005\n",
      "d:  state:   0.533  action:  -0.166  reward:   0.672 c_value   0.395 critic_dvda  -0.461 cl   0.077\n",
      "d:  state:  -1.712  action:  -0.521  reward:   0.413 c_value   0.789 critic_dvda  -0.461 cl   0.141\n",
      "d:  state:  -1.181  action:  -0.437  reward:   0.644 c_value   0.696 critic_dvda  -0.461 cl   0.003\n",
      "d:  state:   1.406  action:  -0.028  reward:   0.327 c_value   0.242 critic_dvda  -0.461 cl   0.007\n",
      "d:  state:   1.428  action:  -0.024  reward:   0.321 c_value   0.238 critic_dvda  -0.461 cl   0.007\n",
      "d:  state:  -2.302  action:  -0.614  reward:   0.260 c_value   0.892 critic_dvda  -0.461 cl   0.400\n",
      "d:  state:  -0.996  action:  -0.408  reward:   0.743 c_value   0.663 critic_dvda  -0.461 cl   0.006\n",
      "d:  state:   1.330  action:  -0.040  reward:   0.348 c_value   0.255 critic_dvda  -0.461 cl   0.009\n",
      "d:  state:   0.234  action:  -0.213  reward:   0.833 c_value   0.448 critic_dvda  -0.461 cl   0.149\n",
      " 200 0.69 0.11\n",
      "global_step  300\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.04774411 -0.04074279]\n",
      " [ 0.04640253 -0.03959794]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.09713399  0.08289   ], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.07997195]\n",
      " [ 0.03431384]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.1354532], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]\n",
      " [-0.6010277]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.99362093 -0.5791373   0.8533904  -0.6010277 ]\n",
      " [-0.24299872 -0.45166948  0.9582735  -0.6010277 ]\n",
      " [-0.9065832  -0.56435686  0.8951599  -0.6010277 ]\n",
      " [ 0.01171622 -0.40841475  0.8499715  -0.6010277 ]\n",
      " [ 1.2138708  -0.20426942  0.33210218 -0.6010277 ]\n",
      " [ 0.5027644  -0.3250268   0.59338784 -0.6010277 ]\n",
      " [-0.18928577 -0.44254816  0.9397243  -0.6010277 ]\n",
      " [ 0.09457583 -0.39434385  0.8070749  -0.6010277 ]\n",
      " [ 0.38715744 -0.34465873  0.6512304  -0.6010277 ]\n",
      " [ 1.4003578  -0.1726009   0.28783613 -0.6010277 ]\n",
      " [-1.376389   -0.6441375   0.65096015 -0.6010277 ]\n",
      " [-0.4436271  -0.4857394   0.9982297  -0.6010277 ]\n",
      " [-0.9200714  -0.5666474   0.88896114 -0.6010277 ]\n",
      " [-0.21930294 -0.44764555  0.95044357 -0.6010277 ]\n",
      " [ 0.00577806 -0.40942314  0.852957   -0.6010277 ]\n",
      " [-0.06008217 -0.42060727  0.88497275 -0.6010277 ]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.994  action:  -0.579  reward:   0.853 c_value   0.841 critic_dvda  -0.601 cl   0.000\n",
      "d:  state:  -0.243  action:  -0.452  reward:   0.958 c_value   0.727 critic_dvda  -0.601 cl   0.053\n",
      "d:  state:  -0.907  action:  -0.564  reward:   0.895 c_value   0.828 critic_dvda  -0.601 cl   0.005\n",
      "d:  state:   0.012  action:  -0.408  reward:   0.850 c_value   0.689 critic_dvda  -0.601 cl   0.026\n",
      "d:  state:   1.214  action:  -0.204  reward:   0.332 c_value   0.507 critic_dvda  -0.601 cl   0.030\n",
      "d:  state:   0.503  action:  -0.325  reward:   0.593 c_value   0.614 critic_dvda  -0.601 cl   0.000\n",
      "d:  state:  -0.189  action:  -0.443  reward:   0.940 c_value   0.719 critic_dvda  -0.601 cl   0.049\n",
      "d:  state:   0.095  action:  -0.394  reward:   0.807 c_value   0.676 critic_dvda  -0.601 cl   0.017\n",
      "d:  state:   0.387  action:  -0.345  reward:   0.651 c_value   0.632 critic_dvda  -0.601 cl   0.000\n",
      "d:  state:   1.400  action:  -0.173  reward:   0.288 c_value   0.478 critic_dvda  -0.601 cl   0.036\n",
      "d:  state:  -1.376  action:  -0.644  reward:   0.651 c_value   0.899 critic_dvda  -0.601 cl   0.062\n",
      "d:  state:  -0.444  action:  -0.486  reward:   0.998 c_value   0.758 critic_dvda  -0.601 cl   0.058\n",
      "d:  state:  -0.920  action:  -0.567  reward:   0.889 c_value   0.830 critic_dvda  -0.601 cl   0.003\n",
      "d:  state:  -0.219  action:  -0.448  reward:   0.950 c_value   0.724 critic_dvda  -0.601 cl   0.051\n",
      "d:  state:   0.006  action:  -0.409  reward:   0.853 c_value   0.690 critic_dvda  -0.601 cl   0.027\n",
      "d:  state:  -0.060  action:  -0.421  reward:   0.885 c_value   0.700 critic_dvda  -0.601 cl   0.034\n",
      " 300 0.73 0.04\n",
      "global_step  400\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.01165294  0.0103732 ]\n",
      " [ 0.03387077 -0.03015101]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.03273193 -0.02913726], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.03723266]\n",
      " [0.01355885]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.04776763], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]\n",
      " [-0.5145937]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.4600511  -0.46050942  0.541292   -0.5145937 ]\n",
      " [-0.48059294 -0.6315346   0.97772413 -0.5145937 ]\n",
      " [-1.8692304  -0.8840127   0.50744575 -0.5145937 ]\n",
      " [ 0.7951493  -0.39958283  0.411966   -0.5145937 ]\n",
      " [-0.2331468  -0.58654463  0.88897574 -0.5145937 ]\n",
      " [ 0.02154645 -0.54023707  0.76010907 -0.5145937 ]\n",
      " [-1.2011862  -0.7625508   0.838644   -0.5145937 ]\n",
      " [-0.7279599  -0.67651016  0.99735993 -0.5145937 ]\n",
      " [ 0.6919348  -0.418349    0.44788212 -0.5145937 ]\n",
      " [ 1.1825651  -0.329144    0.3043895  -0.5145937 ]\n",
      " [-0.6828171  -0.6683024   0.9997894  -0.5145937 ]\n",
      " [ 0.1801292  -0.51140404  0.6764901  -0.5145937 ]\n",
      " [ 0.72554547 -0.412238    0.43581504 -0.5145937 ]\n",
      " [-0.7578885  -0.6819517   0.9942667  -0.5145937 ]\n",
      " [ 0.18510327 -0.51049966  0.6739165  -0.5145937 ]\n",
      " [ 0.8654518  -0.38680065  0.38938746 -0.5145937 ]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:   0.460  action:  -0.461  reward:   0.541 c_value   0.624 critic_dvda  -0.515 cl   0.007\n",
      "d:  state:  -0.481  action:  -0.632  reward:   0.978 c_value   0.767 critic_dvda  -0.515 cl   0.045\n",
      "d:  state:  -1.869  action:  -0.884  reward:   0.507 c_value   0.978 critic_dvda  -0.515 cl   0.221\n",
      "d:  state:   0.795  action:  -0.400  reward:   0.412 c_value   0.573 critic_dvda  -0.515 cl   0.026\n",
      "d:  state:  -0.233  action:  -0.587  reward:   0.889 c_value   0.729 critic_dvda  -0.515 cl   0.026\n",
      "d:  state:   0.022  action:  -0.540  reward:   0.760 c_value   0.690 critic_dvda  -0.515 cl   0.005\n",
      "d:  state:  -1.201  action:  -0.763  reward:   0.839 c_value   0.876 critic_dvda  -0.515 cl   0.001\n",
      "d:  state:  -0.728  action:  -0.677  reward:   0.997 c_value   0.804 critic_dvda  -0.515 cl   0.037\n",
      "d:  state:   0.692  action:  -0.418  reward:   0.448 c_value   0.588 critic_dvda  -0.515 cl   0.020\n",
      "d:  state:   1.183  action:  -0.329  reward:   0.304 c_value   0.514 critic_dvda  -0.515 cl   0.044\n",
      "d:  state:  -0.683  action:  -0.668  reward:   1.000 c_value   0.798 critic_dvda  -0.515 cl   0.041\n",
      "d:  state:   0.180  action:  -0.511  reward:   0.676 c_value   0.666 critic_dvda  -0.515 cl   0.000\n",
      "d:  state:   0.726  action:  -0.412  reward:   0.436 c_value   0.583 critic_dvda  -0.515 cl   0.022\n",
      "d:  state:  -0.758  action:  -0.682  reward:   0.994 c_value   0.809 critic_dvda  -0.515 cl   0.034\n",
      "d:  state:   0.185  action:  -0.510  reward:   0.674 c_value   0.666 critic_dvda  -0.515 cl   0.000\n",
      "d:  state:   0.865  action:  -0.387  reward:   0.389 c_value   0.562 critic_dvda  -0.515 cl   0.030\n",
      " 400 0.67 0.04\n",
      "global_step  500\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.01332369  0.01281792]\n",
      " [ 0.08601491 -0.08274974]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.04438534 -0.04270044], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.06060418]\n",
      " [0.04401582]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.06894831], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]\n",
      " [-0.41926172]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.35002318 -0.70948005  0.8855754  -0.41926172]\n",
      " [-1.4507532  -0.9063939   0.7714102  -0.41926172]\n",
      " [-0.60575914 -0.7552296   0.97814685 -0.41926172]\n",
      " [-1.490227   -0.9134555   0.7503759  -0.41926172]\n",
      " [ 1.117982   -0.44686294  0.28996098 -0.41926172]\n",
      " [ 0.89776087 -0.48625916  0.34299296 -0.41926172]\n",
      " [-0.79659295 -0.7893686   0.9999478  -0.41926172]\n",
      " [ 0.28064474 -0.59665745  0.5650806  -0.41926172]\n",
      " [ 0.3667505  -0.58125365  0.5266729  -0.41926172]\n",
      " [ 0.25888294 -0.6005505   0.5751669  -0.41926172]\n",
      " [ 2.5967753  -0.18231595  0.11463488 -0.41926172]\n",
      " [ 1.5891124  -0.36258063  0.20793886 -0.41926172]\n",
      " [ 0.16599572 -0.6171674   0.6198304  -0.41926172]\n",
      " [ 0.10418692 -0.6282247   0.65086085 -0.41926172]\n",
      " [ 0.19694051 -0.6116316   0.60467213 -0.41926172]\n",
      " [ 0.7347198  -0.5154262   0.39018828 -0.41926172]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.350  action:  -0.709  reward:   0.886 c_value   0.708 critic_dvda  -0.419 cl   0.032\n",
      "d:  state:  -1.451  action:  -0.906  reward:   0.771 c_value   0.888 critic_dvda  -0.419 cl   0.014\n",
      "d:  state:  -0.606  action:  -0.755  reward:   0.978 c_value   0.750 critic_dvda  -0.419 cl   0.052\n",
      "d:  state:  -1.490  action:  -0.913  reward:   0.750 c_value   0.894 critic_dvda  -0.419 cl   0.021\n",
      "d:  state:   1.118  action:  -0.447  reward:   0.290 c_value   0.468 critic_dvda  -0.419 cl   0.032\n",
      "d:  state:   0.898  action:  -0.486  reward:   0.343 c_value   0.504 critic_dvda  -0.419 cl   0.026\n",
      "d:  state:  -0.797  action:  -0.789  reward:   1.000 c_value   0.781 critic_dvda  -0.419 cl   0.048\n",
      "d:  state:   0.281  action:  -0.597  reward:   0.565 c_value   0.605 critic_dvda  -0.419 cl   0.002\n",
      "d:  state:   0.367  action:  -0.581  reward:   0.527 c_value   0.591 critic_dvda  -0.419 cl   0.004\n",
      "d:  state:   0.259  action:  -0.601  reward:   0.575 c_value   0.609 critic_dvda  -0.419 cl   0.001\n",
      "d:  state:   2.597  action:  -0.182  reward:   0.115 c_value   0.227 critic_dvda  -0.419 cl   0.013\n",
      "d:  state:   1.589  action:  -0.363  reward:   0.208 c_value   0.392 critic_dvda  -0.419 cl   0.034\n",
      "d:  state:   0.166  action:  -0.617  reward:   0.620 c_value   0.624 critic_dvda  -0.419 cl   0.000\n",
      "d:  state:   0.104  action:  -0.628  reward:   0.651 c_value   0.634 critic_dvda  -0.419 cl   0.000\n",
      "d:  state:   0.197  action:  -0.612  reward:   0.605 c_value   0.619 critic_dvda  -0.419 cl   0.000\n",
      "d:  state:   0.735  action:  -0.515  reward:   0.390 c_value   0.531 critic_dvda  -0.419 cl   0.020\n",
      " 500 0.63 0.03\n",
      "global_step  600\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.00699901  0.0071887 ]\n",
      " [ 0.03684181 -0.03784026]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.01899222 -0.01950693], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.02744663]\n",
      " [0.02151866]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.0308004], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]\n",
      " [-0.36309606]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.2198264  -0.7764187   0.76347834 -0.36309606]\n",
      " [-0.953221   -0.91493595  0.99853635 -0.36309606]\n",
      " [-0.32533273 -0.7963458   0.8184289  -0.36309606]\n",
      " [ 0.01747068 -0.73160005  0.6405711  -0.36309606]\n",
      " [ 1.9319938  -0.37000144  0.15875076 -0.36309606]\n",
      " [ 1.4866244  -0.45411897  0.20979822 -0.36309606]\n",
      " [-0.65168184 -0.8579838   0.959177   -0.36309606]\n",
      " [ 0.12616779 -0.7110703   0.5879007  -0.36309606]\n",
      " [ 1.6697284  -0.41953588  0.1863924  -0.36309606]\n",
      " [-1.3642707  -0.9925715   0.87861085 -0.36309606]\n",
      " [-1.8592626  -1.0860612   0.6258448  -0.36309606]\n",
      " [ 1.0146587  -0.5432598   0.29179102 -0.36309606]\n",
      " [-1.450644   -1.0088849   0.83671427 -0.36309606]\n",
      " [-0.30840352 -0.79314834  0.80973136 -0.36309606]\n",
      " [ 0.04914148 -0.72561836  0.62490124 -0.36309606]\n",
      " [ 0.3455452  -0.6696362   0.4924669  -0.36309606]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.220  action:  -0.776  reward:   0.763 c_value   0.668 critic_dvda  -0.363 cl   0.009\n",
      "d:  state:  -0.953  action:  -0.915  reward:   0.999 c_value   0.806 critic_dvda  -0.363 cl   0.037\n",
      "d:  state:  -0.325  action:  -0.796  reward:   0.818 c_value   0.688 critic_dvda  -0.363 cl   0.017\n",
      "d:  state:   0.017  action:  -0.732  reward:   0.641 c_value   0.624 critic_dvda  -0.363 cl   0.000\n",
      "d:  state:   1.932  action:  -0.370  reward:   0.159 c_value   0.266 critic_dvda  -0.363 cl   0.011\n",
      "d:  state:   1.487  action:  -0.454  reward:   0.210 c_value   0.349 critic_dvda  -0.363 cl   0.019\n",
      "d:  state:  -0.652  action:  -0.858  reward:   0.959 c_value   0.749 critic_dvda  -0.363 cl   0.044\n",
      "d:  state:   0.126  action:  -0.711  reward:   0.588 c_value   0.604 critic_dvda  -0.363 cl   0.000\n",
      "d:  state:   1.670  action:  -0.420  reward:   0.186 c_value   0.315 critic_dvda  -0.363 cl   0.016\n",
      "d:  state:  -1.364  action:  -0.993  reward:   0.879 c_value   0.883 critic_dvda  -0.363 cl   0.000\n",
      "d:  state:  -1.859  action:  -1.086  reward:   0.626 c_value   0.975 critic_dvda  -0.363 cl   0.122\n",
      "d:  state:   1.015  action:  -0.543  reward:   0.292 c_value   0.437 critic_dvda  -0.363 cl   0.021\n",
      "d:  state:  -1.451  action:  -1.009  reward:   0.837 c_value   0.899 critic_dvda  -0.363 cl   0.004\n",
      "d:  state:  -0.308  action:  -0.793  reward:   0.810 c_value   0.685 critic_dvda  -0.363 cl   0.016\n",
      "d:  state:   0.049  action:  -0.726  reward:   0.625 c_value   0.618 critic_dvda  -0.363 cl   0.000\n",
      "d:  state:   0.346  action:  -0.670  reward:   0.492 c_value   0.563 critic_dvda  -0.363 cl   0.005\n",
      " 600 0.60 0.03\n",
      "global_step  700\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.10823675  0.11903003]\n",
      " [-0.18709584  0.20575285]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.09114752 -0.10023668], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.12628019]\n",
      " [-0.09017421]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.15445773], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]\n",
      " [-0.30736572]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-2.4963984  -1.2684077   0.39873025 -0.30736572]\n",
      " [ 0.14267397 -0.78714806  0.536317   -0.30736572]\n",
      " [ 1.5262809  -0.5348344   0.19054165 -0.30736572]\n",
      " [-0.08189548 -0.82810044  0.6423343  -0.30736572]\n",
      " [ 1.8771955  -0.4708418   0.1535325  -0.30736572]\n",
      " [ 0.3385386  -0.75143033  0.45703167 -0.30736572]\n",
      " [-0.6813622  -0.9374188   0.9384693  -0.30736572]\n",
      " [-0.08817921 -0.82924634  0.64550257 -0.30736572]\n",
      " [-0.00700551 -0.8144435   0.605343   -0.30736572]\n",
      " [ 0.8849314  -0.6517905   0.29748496 -0.30736572]\n",
      " [-0.5515864  -0.913753    0.8840446  -0.30736572]\n",
      " [ 0.16403802 -0.7832521   0.52704847 -0.30736572]\n",
      " [-0.41799966 -0.8893922   0.8181894  -0.30736572]\n",
      " [-2.1396947  -1.2033595   0.53284353 -0.30736572]\n",
      " [ 0.15911144 -0.78415054  0.5291725  -0.30736572]\n",
      " [-1.7522066  -1.1326973   0.72265214 -0.30736572]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -2.496  action:  -1.268  reward:   0.399 c_value   1.102 critic_dvda  -0.307 cl   0.495\n",
      "d:  state:   0.143  action:  -0.787  reward:   0.536 c_value   0.563 critic_dvda  -0.307 cl   0.001\n",
      "d:  state:   1.526  action:  -0.535  reward:   0.191 c_value   0.280 critic_dvda  -0.307 cl   0.008\n",
      "d:  state:  -0.082  action:  -0.828  reward:   0.642 c_value   0.609 critic_dvda  -0.307 cl   0.001\n",
      "d:  state:   1.877  action:  -0.471  reward:   0.154 c_value   0.209 critic_dvda  -0.307 cl   0.003\n",
      "d:  state:   0.339  action:  -0.751  reward:   0.457 c_value   0.523 critic_dvda  -0.307 cl   0.004\n",
      "d:  state:  -0.681  action:  -0.937  reward:   0.938 c_value   0.732 critic_dvda  -0.307 cl   0.043\n",
      "d:  state:  -0.088  action:  -0.829  reward:   0.646 c_value   0.610 critic_dvda  -0.307 cl   0.001\n",
      "d:  state:  -0.007  action:  -0.814  reward:   0.605 c_value   0.594 critic_dvda  -0.307 cl   0.000\n",
      "d:  state:   0.885  action:  -0.652  reward:   0.297 c_value   0.411 critic_dvda  -0.307 cl   0.013\n",
      "d:  state:  -0.552  action:  -0.914  reward:   0.884 c_value   0.705 critic_dvda  -0.307 cl   0.032\n",
      "d:  state:   0.164  action:  -0.783  reward:   0.527 c_value   0.559 critic_dvda  -0.307 cl   0.001\n",
      "d:  state:  -0.418  action:  -0.889  reward:   0.818 c_value   0.678 critic_dvda  -0.307 cl   0.020\n",
      "d:  state:  -2.140  action:  -1.203  reward:   0.533 c_value   1.030 critic_dvda  -0.307 cl   0.247\n",
      "d:  state:   0.159  action:  -0.784  reward:   0.529 c_value   0.560 critic_dvda  -0.307 cl   0.001\n",
      "d:  state:  -1.752  action:  -1.133  reward:   0.723 c_value   0.950 critic_dvda  -0.307 cl   0.052\n",
      " 700 0.59 0.03\n",
      "global_step  800\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.03012165  0.03415659]\n",
      " [ 0.09547643 -0.10826597]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.05481355 -0.06215609], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.08938364]\n",
      " [0.0698539 ]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.09468761], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]\n",
      " [-0.2838834]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.25100994 -0.8361337   0.45832005 -0.2838834 ]\n",
      " [ 1.0666614  -0.679329    0.24700616 -0.2838834 ]\n",
      " [-0.19676873 -0.9222169   0.65518993 -0.2838834 ]\n",
      " [ 1.0140573  -0.6894418   0.2562848  -0.2838834 ]\n",
      " [ 1.2287933  -0.6481599   0.22109449 -0.2838834 ]\n",
      " [ 0.3049627  -0.8257616   0.438878   -0.2838834 ]\n",
      " [ 0.9182364  -0.7078629   0.27440873 -0.2838834 ]\n",
      " [-0.24200162 -0.9309127   0.6781507  -0.2838834 ]\n",
      " [ 0.72848225 -0.74434215  0.31553587 -0.2838834 ]\n",
      " [ 0.9834988  -0.69531655  0.26188836 -0.2838834 ]\n",
      " [-0.69479036 -1.017959    0.90543795 -0.2838834 ]\n",
      " [ 0.89524025 -0.7122838   0.27900746 -0.2838834 ]\n",
      " [ 1.2415324  -0.6457109   0.21921712 -0.2838834 ]\n",
      " [ 1.1394255  -0.6653404   0.23489727 -0.2838834 ]\n",
      " [ 0.12507491 -0.8603441   0.5073436  -0.2838834 ]\n",
      " [-0.7899724  -1.0362573   0.94281256 -0.2838834 ]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:   0.251  action:  -0.836  reward:   0.458 c_value   0.532 critic_dvda  -0.284 cl   0.005\n",
      "d:  state:   1.067  action:  -0.679  reward:   0.247 c_value   0.356 critic_dvda  -0.284 cl   0.012\n",
      "d:  state:  -0.197  action:  -0.922  reward:   0.655 c_value   0.628 critic_dvda  -0.284 cl   0.001\n",
      "d:  state:   1.014  action:  -0.689  reward:   0.256 c_value   0.367 critic_dvda  -0.284 cl   0.012\n",
      "d:  state:   1.229  action:  -0.648  reward:   0.221 c_value   0.321 critic_dvda  -0.284 cl   0.010\n",
      "d:  state:   0.305  action:  -0.826  reward:   0.439 c_value   0.520 critic_dvda  -0.284 cl   0.007\n",
      "d:  state:   0.918  action:  -0.708  reward:   0.274 c_value   0.388 critic_dvda  -0.284 cl   0.013\n",
      "d:  state:  -0.242  action:  -0.931  reward:   0.678 c_value   0.638 critic_dvda  -0.284 cl   0.002\n",
      "d:  state:   0.728  action:  -0.744  reward:   0.316 c_value   0.429 critic_dvda  -0.284 cl   0.013\n",
      "d:  state:   0.983  action:  -0.695  reward:   0.262 c_value   0.374 critic_dvda  -0.284 cl   0.012\n",
      "d:  state:  -0.695  action:  -1.018  reward:   0.905 c_value   0.736 critic_dvda  -0.284 cl   0.029\n",
      "d:  state:   0.895  action:  -0.712  reward:   0.279 c_value   0.393 critic_dvda  -0.284 cl   0.013\n",
      "d:  state:   1.242  action:  -0.646  reward:   0.219 c_value   0.318 critic_dvda  -0.284 cl   0.010\n",
      "d:  state:   1.139  action:  -0.665  reward:   0.235 c_value   0.340 critic_dvda  -0.284 cl   0.011\n",
      "d:  state:   0.125  action:  -0.860  reward:   0.507 c_value   0.559 critic_dvda  -0.284 cl   0.003\n",
      "d:  state:  -0.790  action:  -1.036  reward:   0.943 c_value   0.757 critic_dvda  -0.284 cl   0.035\n",
      " 800 0.54 0.03\n",
      "global_step  900\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.08849429  0.10683326]\n",
      " [-0.16965058  0.20480785]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.06239349 -0.07532351], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.1008296 ]\n",
      " [-0.08655918]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.1121462], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]\n",
      " [-0.23625804]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.7352021  -1.077691    0.8950159  -0.23625804]\n",
      " [-0.4184117  -1.0232965   0.7321258  -0.23625804]\n",
      " [-1.798298   -1.2602295   0.77548367 -0.23625804]\n",
      " [ 0.4842639  -0.8683029   0.35342723 -0.23625804]\n",
      " [ 0.46443245 -0.87170804  0.3590314  -0.23625804]\n",
      " [ 0.80565715 -0.81311816  0.27620998 -0.23625804]\n",
      " [ 0.28463972 -0.9025793   0.41502574 -0.23625804]\n",
      " [-0.5871553  -1.0522705   0.8221438  -0.23625804]\n",
      " [ 0.8897101  -0.79868585  0.2596943  -0.23625804]\n",
      " [-0.414352   -1.0225995   0.72994566 -0.23625804]\n",
      " [-0.35178027 -1.0118556   0.6965246  -0.23625804]\n",
      " [-0.24318644 -0.99320954  0.6399858  -0.23625804]\n",
      " [ 0.92760456 -0.7921792   0.25267482 -0.23625804]\n",
      " [ 0.9008744  -0.7967689   0.25759965 -0.23625804]\n",
      " [-3.084844   -1.4811355   0.2799645  -0.23625804]\n",
      " [-0.7122997  -1.0737585   0.8844451  -0.23625804]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.735  action:  -1.078  reward:   0.895 c_value   0.711 critic_dvda  -0.236 cl   0.034\n",
      "d:  state:  -0.418  action:  -1.023  reward:   0.732 c_value   0.639 critic_dvda  -0.236 cl   0.009\n",
      "d:  state:  -1.798  action:  -1.260  reward:   0.775 c_value   0.952 critic_dvda  -0.236 cl   0.031\n",
      "d:  state:   0.484  action:  -0.868  reward:   0.353 c_value   0.435 critic_dvda  -0.236 cl   0.007\n",
      "d:  state:   0.464  action:  -0.872  reward:   0.359 c_value   0.439 critic_dvda  -0.236 cl   0.006\n",
      "d:  state:   0.806  action:  -0.813  reward:   0.276 c_value   0.362 critic_dvda  -0.236 cl   0.007\n",
      "d:  state:   0.285  action:  -0.903  reward:   0.415 c_value   0.480 critic_dvda  -0.236 cl   0.004\n",
      "d:  state:  -0.587  action:  -1.052  reward:   0.822 c_value   0.678 critic_dvda  -0.236 cl   0.021\n",
      "d:  state:   0.890  action:  -0.799  reward:   0.260 c_value   0.343 critic_dvda  -0.236 cl   0.007\n",
      "d:  state:  -0.414  action:  -1.023  reward:   0.730 c_value   0.638 critic_dvda  -0.236 cl   0.008\n",
      "d:  state:  -0.352  action:  -1.012  reward:   0.697 c_value   0.624 critic_dvda  -0.236 cl   0.005\n",
      "d:  state:  -0.243  action:  -0.993  reward:   0.640 c_value   0.600 critic_dvda  -0.236 cl   0.002\n",
      "d:  state:   0.928  action:  -0.792  reward:   0.253 c_value   0.335 critic_dvda  -0.236 cl   0.007\n",
      "d:  state:   0.901  action:  -0.797  reward:   0.258 c_value   0.341 critic_dvda  -0.236 cl   0.007\n",
      "d:  state:  -3.085  action:  -1.481  reward:   0.280 c_value   1.243 critic_dvda  -0.236 cl   0.928\n",
      "d:  state:  -0.712  action:  -1.074  reward:   0.884 c_value   0.706 critic_dvda  -0.236 cl   0.032\n",
      " 900 0.55 0.02\n",
      "global_step  1000\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.11517523  0.14374228]\n",
      " [-0.11334007  0.14145198]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.09434475 -0.11774518], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.16985057]\n",
      " [-0.03638282]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.17262274], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]\n",
      " [-0.21634395]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.52057487 -0.92436385  0.32385015 -0.21634395]\n",
      " [ 0.6179448  -0.90760165  0.3005447  -0.21634395]\n",
      " [-2.8246734  -1.5002477   0.36309457 -0.21634395]\n",
      " [ 0.20159511 -0.9792762   0.41763124 -0.21634395]\n",
      " [ 0.7802683  -0.8796577   0.26628646 -0.21634395]\n",
      " [ 0.15574275 -0.9871697   0.4336046  -0.21634395]\n",
      " [ 0.35198903 -0.9533859   0.369822   -0.21634395]\n",
      " [ 0.5697351  -0.91590095  0.3118068  -0.21634395]\n",
      " [-0.8725996  -1.1641986   0.92163354 -0.21634395]\n",
      " [ 0.73013896 -0.8882875   0.2762962  -0.21634395]\n",
      " [-0.39786232 -1.0824727   0.6808783  -0.21634395]\n",
      " [-1.3842273  -1.2522752   0.9828866  -0.21634395]\n",
      " [-0.06347983 -1.0249088   0.5196572  -0.21634395]\n",
      " [ 0.24001406 -0.9726624   0.40476263 -0.21634395]\n",
      " [ 0.3070465  -0.96112275  0.38339832 -0.21634395]\n",
      " [ 0.376352   -0.9491918   0.3627043  -0.21634395]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:   0.521  action:  -0.924  reward:   0.324 c_value   0.409 critic_dvda  -0.216 cl   0.007\n",
      "d:  state:   0.618  action:  -0.908  reward:   0.301 c_value   0.386 critic_dvda  -0.216 cl   0.007\n",
      "d:  state:  -2.825  action:  -1.500  reward:   0.363 c_value   1.209 critic_dvda  -0.216 cl   0.716\n",
      "d:  state:   0.202  action:  -0.979  reward:   0.418 c_value   0.485 critic_dvda  -0.216 cl   0.005\n",
      "d:  state:   0.780  action:  -0.880  reward:   0.266 c_value   0.347 critic_dvda  -0.216 cl   0.006\n",
      "d:  state:   0.156  action:  -0.987  reward:   0.434 c_value   0.496 critic_dvda  -0.216 cl   0.004\n",
      "d:  state:   0.352  action:  -0.953  reward:   0.370 c_value   0.449 critic_dvda  -0.216 cl   0.006\n",
      "d:  state:   0.570  action:  -0.916  reward:   0.312 c_value   0.397 critic_dvda  -0.216 cl   0.007\n",
      "d:  state:  -0.873  action:  -1.164  reward:   0.922 c_value   0.742 critic_dvda  -0.216 cl   0.032\n",
      "d:  state:   0.730  action:  -0.888  reward:   0.276 c_value   0.359 critic_dvda  -0.216 cl   0.007\n",
      "d:  state:  -0.398  action:  -1.082  reward:   0.681 c_value   0.629 critic_dvda  -0.216 cl   0.003\n",
      "d:  state:  -1.384  action:  -1.252  reward:   0.983 c_value   0.864 critic_dvda  -0.216 cl   0.014\n",
      "d:  state:  -0.063  action:  -1.025  reward:   0.520 c_value   0.549 critic_dvda  -0.216 cl   0.001\n",
      "d:  state:   0.240  action:  -0.973  reward:   0.405 c_value   0.476 critic_dvda  -0.216 cl   0.005\n",
      "d:  state:   0.307  action:  -0.961  reward:   0.383 c_value   0.460 critic_dvda  -0.216 cl   0.006\n",
      "d:  state:   0.376  action:  -0.949  reward:   0.363 c_value   0.443 critic_dvda  -0.216 cl   0.007\n",
      "1000 0.52 0.02\n",
      "global_step  1100\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.02979351  0.03829312]\n",
      " [-0.03593538  0.04618717]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.02178222 -0.02799633], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.04145608]\n",
      " [-0.01326413]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.04081884], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]\n",
      " [-0.18716419]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.49867442 -0.98247075  0.31310752 -0.18716419]\n",
      " [ 1.5713949  -0.79007924  0.15205519 -0.18716419]\n",
      " [ 0.14489828 -1.0459203   0.41355658 -0.18716419]\n",
      " [-2.412357   -1.5045619   0.5482179  -0.18716419]\n",
      " [ 1.0248024  -0.88811016  0.21462753 -0.18716419]\n",
      " [-0.82547534 -1.2199559   0.8653401  -0.18716419]\n",
      " [-0.38114303 -1.1402653   0.6344104  -0.18716419]\n",
      " [-0.23950642 -1.1148629   0.5661716  -0.18716419]\n",
      " [ 0.10416834 -1.0532252   0.42743087 -0.18716419]\n",
      " [ 0.86432153 -0.9168923   0.23965167 -0.18716419]\n",
      " [-1.6598722  -1.3696043   0.9222919  -0.18716419]\n",
      " [-0.7828671  -1.2123142   0.8442916  -0.18716419]\n",
      " [-0.8031466  -1.2159513   0.8544031  -0.18716419]\n",
      " [ 0.6314442  -0.95865864  0.28341237 -0.18716419]\n",
      " [ 0.78998566 -0.93022436  0.2525812  -0.18716419]\n",
      " [-1.5635191  -1.3523235   0.95730096 -0.18716419]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:   0.499  action:  -0.982  reward:   0.313 c_value   0.386 critic_dvda  -0.187 cl   0.005\n",
      "d:  state:   1.571  action:  -0.790  reward:   0.152 c_value   0.129 critic_dvda  -0.187 cl   0.001\n",
      "d:  state:   0.145  action:  -1.046  reward:   0.414 c_value   0.471 critic_dvda  -0.187 cl   0.003\n",
      "d:  state:  -2.412  action:  -1.505  reward:   0.548 c_value   1.083 critic_dvda  -0.187 cl   0.286\n",
      "d:  state:   1.025  action:  -0.888  reward:   0.215 c_value   0.260 critic_dvda  -0.187 cl   0.002\n",
      "d:  state:  -0.825  action:  -1.220  reward:   0.865 c_value   0.703 critic_dvda  -0.187 cl   0.026\n",
      "d:  state:  -0.381  action:  -1.140  reward:   0.634 c_value   0.597 critic_dvda  -0.187 cl   0.001\n",
      "d:  state:  -0.240  action:  -1.115  reward:   0.566 c_value   0.563 critic_dvda  -0.187 cl   0.000\n",
      "d:  state:   0.104  action:  -1.053  reward:   0.427 c_value   0.481 critic_dvda  -0.187 cl   0.003\n",
      "d:  state:   0.864  action:  -0.917  reward:   0.240 c_value   0.298 critic_dvda  -0.187 cl   0.003\n",
      "d:  state:  -1.660  action:  -1.370  reward:   0.922 c_value   0.903 critic_dvda  -0.187 cl   0.000\n",
      "d:  state:  -0.783  action:  -1.212  reward:   0.844 c_value   0.693 critic_dvda  -0.187 cl   0.023\n",
      "d:  state:  -0.803  action:  -1.216  reward:   0.854 c_value   0.698 critic_dvda  -0.187 cl   0.025\n",
      "d:  state:   0.631  action:  -0.959  reward:   0.283 c_value   0.354 critic_dvda  -0.187 cl   0.005\n",
      "d:  state:   0.790  action:  -0.930  reward:   0.253 c_value   0.316 critic_dvda  -0.187 cl   0.004\n",
      "d:  state:  -1.564  action:  -1.352  reward:   0.957 c_value   0.880 critic_dvda  -0.187 cl   0.006\n",
      "1100 0.52 0.02\n",
      "global_step  1200\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.08340816  0.10968688]\n",
      " [-0.19667175  0.2586355 ]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.04347457 -0.05717175], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.08419264]\n",
      " [-0.11046572]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.08266538], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]\n",
      " [-0.17061606]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.8086874  -1.2687577   0.8253108  -0.17061606]\n",
      " [-2.7061868  -1.6006372   0.4499964  -0.17061606]\n",
      " [-2.5875652  -1.5798899   0.49617708 -0.17061606]\n",
      " [-1.2379144  -1.3438311   0.98890615 -0.17061606]\n",
      " [ 0.6883919  -1.0069131   0.2581272  -0.17061606]\n",
      " [ 1.4390198  -0.8756255   0.15729249 -0.17061606]\n",
      " [-1.3592806  -1.3650584   0.9999666  -0.17061606]\n",
      " [ 1.4129806  -0.8801798   0.15978053 -0.17061606]\n",
      " [ 0.3303272  -1.0695398   0.33788034 -0.17061606]\n",
      " [ 0.14151978 -1.1025629   0.39250442 -0.17061606]\n",
      " [-0.09512427 -1.1439528   0.4761811  -0.17061606]\n",
      " [ 0.19955255 -1.0924128   0.37464765 -0.17061606]\n",
      " [-0.9898001  -1.300435    0.91199774 -0.17061606]\n",
      " [-0.86328334 -1.2783067   0.8530644  -0.17061606]\n",
      " [ 0.2028812  -1.0918306   0.37365317 -0.17061606]\n",
      " [ 2.3884358  -0.7095691   0.09436068 -0.17061606]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.809  action:  -1.269  reward:   0.825 c_value   0.689 critic_dvda  -0.171 cl   0.019\n",
      "d:  state:  -2.706  action:  -1.601  reward:   0.450 c_value   1.151 critic_dvda  -0.171 cl   0.492\n",
      "d:  state:  -2.588  action:  -1.580  reward:   0.496 c_value   1.122 critic_dvda  -0.171 cl   0.392\n",
      "d:  state:  -1.238  action:  -1.344  reward:   0.989 c_value   0.794 critic_dvda  -0.171 cl   0.038\n",
      "d:  state:   0.688  action:  -1.007  reward:   0.258 c_value   0.325 critic_dvda  -0.171 cl   0.004\n",
      "d:  state:   1.439  action:  -0.876  reward:   0.157 c_value   0.142 critic_dvda  -0.171 cl   0.000\n",
      "d:  state:  -1.359  action:  -1.365  reward:   1.000 c_value   0.823 critic_dvda  -0.171 cl   0.031\n",
      "d:  state:   1.413  action:  -0.880  reward:   0.160 c_value   0.148 critic_dvda  -0.171 cl   0.000\n",
      "d:  state:   0.330  action:  -1.070  reward:   0.338 c_value   0.412 critic_dvda  -0.171 cl   0.005\n",
      "d:  state:   0.142  action:  -1.103  reward:   0.393 c_value   0.458 critic_dvda  -0.171 cl   0.004\n",
      "d:  state:  -0.095  action:  -1.144  reward:   0.476 c_value   0.515 critic_dvda  -0.171 cl   0.002\n",
      "d:  state:   0.200  action:  -1.092  reward:   0.375 c_value   0.444 critic_dvda  -0.171 cl   0.005\n",
      "d:  state:  -0.990  action:  -1.300  reward:   0.912 c_value   0.733 critic_dvda  -0.171 cl   0.032\n",
      "d:  state:  -0.863  action:  -1.278  reward:   0.853 c_value   0.702 critic_dvda  -0.171 cl   0.023\n",
      "d:  state:   0.203  action:  -1.092  reward:   0.374 c_value   0.443 critic_dvda  -0.171 cl   0.005\n",
      "d:  state:   2.388  action:  -0.710  reward:   0.094 c_value  -0.089 critic_dvda  -0.171 cl   0.034\n",
      "1200 0.47 0.02\n",
      "global_step  1300\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.06051425 -0.08095756]\n",
      " [ 0.03028972 -0.04052239]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.04676135  0.06255855], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.0984972 ]\n",
      " [-0.00850862]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.09022005], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]\n",
      " [-0.15362191]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-1.5530653  -1.4549243   0.9904603  -0.15362191]\n",
      " [-1.3952445  -1.4268898   0.99899954 -0.15362191]\n",
      " [-0.0076816  -1.1804097   0.42100102 -0.15362191]\n",
      " [ 0.13913755 -1.1543294   0.37410358 -0.15362191]\n",
      " [-1.5400655  -1.4526151   0.99241054 -0.15362191]\n",
      " [-0.37805253 -1.2462007   0.57022905 -0.15362191]\n",
      " [ 2.10056    -0.8059117   0.10584724 -0.15362191]\n",
      " [-0.4173531  -1.2531818   0.5887168  -0.15362191]\n",
      " [-0.37581334 -1.2458029   0.56919026 -0.15362191]\n",
      " [-1.439682   -1.4347835   0.99997604 -0.15362191]\n",
      " [-0.6745966  -1.2988774   0.7195661  -0.15362191]\n",
      " [ 2.1968544  -0.78880644  0.10086573 -0.15362191]\n",
      " [ 0.01591397 -1.1762183   0.41302186 -0.15362191]\n",
      " [ 0.63243055 -1.0667032   0.25726414 -0.15362191]\n",
      " [ 0.7246641  -1.0503193   0.24093099 -0.15362191]\n",
      " [-0.31004018 -1.2341193   0.53939694 -0.15362191]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -1.553  action:  -1.455  reward:   0.990 c_value   0.851 critic_dvda  -0.154 cl   0.019\n",
      "d:  state:  -1.395  action:  -1.427  reward:   0.999 c_value   0.813 critic_dvda  -0.154 cl   0.035\n",
      "d:  state:  -0.008  action:  -1.180  reward:   0.421 c_value   0.477 critic_dvda  -0.154 cl   0.003\n",
      "d:  state:   0.139  action:  -1.154  reward:   0.374 c_value   0.442 critic_dvda  -0.154 cl   0.005\n",
      "d:  state:  -1.540  action:  -1.453  reward:   0.992 c_value   0.848 critic_dvda  -0.154 cl   0.021\n",
      "d:  state:  -0.378  action:  -1.246  reward:   0.570 c_value   0.567 critic_dvda  -0.154 cl   0.000\n",
      "d:  state:   2.101  action:  -0.806  reward:   0.106 c_value  -0.033 critic_dvda  -0.154 cl   0.019\n",
      "d:  state:  -0.417  action:  -1.253  reward:   0.589 c_value   0.576 critic_dvda  -0.154 cl   0.000\n",
      "d:  state:  -0.376  action:  -1.246  reward:   0.569 c_value   0.566 critic_dvda  -0.154 cl   0.000\n",
      "d:  state:  -1.440  action:  -1.435  reward:   1.000 c_value   0.824 critic_dvda  -0.154 cl   0.031\n",
      "d:  state:  -0.675  action:  -1.299  reward:   0.720 c_value   0.639 critic_dvda  -0.154 cl   0.007\n",
      "d:  state:   2.197  action:  -0.789  reward:   0.101 c_value  -0.056 critic_dvda  -0.154 cl   0.025\n",
      "d:  state:   0.016  action:  -1.176  reward:   0.413 c_value   0.472 critic_dvda  -0.154 cl   0.003\n",
      "d:  state:   0.632  action:  -1.067  reward:   0.257 c_value   0.322 critic_dvda  -0.154 cl   0.004\n",
      "d:  state:   0.725  action:  -1.050  reward:   0.241 c_value   0.300 critic_dvda  -0.154 cl   0.004\n",
      "d:  state:  -0.310  action:  -1.234  reward:   0.539 c_value   0.551 critic_dvda  -0.154 cl   0.000\n",
      "1300 0.47 0.01\n",
      "global_step  1400\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.0106851   0.01446543]\n",
      " [ 0.02649223 -0.03586503]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.01254565 -0.01698422], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.02820884]\n",
      " [0.02695934]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.02443222], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]\n",
      " [-0.14297143]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.6854042  -1.3512051   0.692861   -0.14297143]\n",
      " [ 0.9377045  -1.061347    0.20015183 -0.14297143]\n",
      " [ 0.7322131  -1.098044    0.22989346 -0.14297143]\n",
      " [-0.5422052  -1.3256323   0.6196716  -0.14297143]\n",
      " [-1.871729   -1.5630616   0.91301227 -0.14297143]\n",
      " [ 0.4692388  -1.1450065   0.2773319  -0.14297143]\n",
      " [-1.980063   -1.5824081   0.8634613  -0.14297143]\n",
      " [ 1.3173668  -0.99354607  0.15772079 -0.14297143]\n",
      " [-0.31891274 -1.2857563   0.51685286 -0.14297143]\n",
      " [ 0.35903594 -1.1646868   0.30104786 -0.14297143]\n",
      " [-1.2082121  -1.4445693   0.94709104 -0.14297143]\n",
      " [ 0.14136125 -1.2035596   0.3560224  -0.14297143]\n",
      " [-0.49466792 -1.3171431   0.596493   -0.14297143]\n",
      " [-0.83652    -1.3781917   0.77315116 -0.14297143]\n",
      " [ 0.6145267  -1.1190608   0.24966761 -0.14297143]\n",
      " [ 0.47206363 -1.1445022   0.27675647 -0.14297143]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.685  action:  -1.351  reward:   0.693 c_value   0.632 critic_dvda  -0.143 cl   0.004\n",
      "d:  state:   0.938  action:  -1.061  reward:   0.200 c_value   0.240 critic_dvda  -0.143 cl   0.002\n",
      "d:  state:   0.732  action:  -1.098  reward:   0.230 c_value   0.290 critic_dvda  -0.143 cl   0.004\n",
      "d:  state:  -0.542  action:  -1.326  reward:   0.620 c_value   0.598 critic_dvda  -0.143 cl   0.000\n",
      "d:  state:  -1.872  action:  -1.563  reward:   0.913 c_value   0.919 critic_dvda  -0.143 cl   0.000\n",
      "d:  state:   0.469  action:  -1.145  reward:   0.277 c_value   0.353 critic_dvda  -0.143 cl   0.006\n",
      "d:  state:  -1.980  action:  -1.582  reward:   0.863 c_value   0.945 critic_dvda  -0.143 cl   0.007\n",
      "d:  state:   1.317  action:  -0.994  reward:   0.158 c_value   0.148 critic_dvda  -0.143 cl   0.000\n",
      "d:  state:  -0.319  action:  -1.286  reward:   0.517 c_value   0.544 critic_dvda  -0.143 cl   0.001\n",
      "d:  state:   0.359  action:  -1.165  reward:   0.301 c_value   0.380 critic_dvda  -0.143 cl   0.006\n",
      "d:  state:  -1.208  action:  -1.445  reward:   0.947 c_value   0.759 critic_dvda  -0.143 cl   0.035\n",
      "d:  state:   0.141  action:  -1.204  reward:   0.356 c_value   0.433 critic_dvda  -0.143 cl   0.006\n",
      "d:  state:  -0.495  action:  -1.317  reward:   0.596 c_value   0.586 critic_dvda  -0.143 cl   0.000\n",
      "d:  state:  -0.837  action:  -1.378  reward:   0.773 c_value   0.669 critic_dvda  -0.143 cl   0.011\n",
      "d:  state:   0.615  action:  -1.119  reward:   0.250 c_value   0.318 critic_dvda  -0.143 cl   0.005\n",
      "d:  state:   0.472  action:  -1.145  reward:   0.277 c_value   0.353 critic_dvda  -0.143 cl   0.006\n",
      "1400 0.43 0.01\n",
      "global_step  1500\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.00402078 -0.00557041]\n",
      " [ 0.019248   -0.0266662 ]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.00039014  0.00054051], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.00073689]\n",
      " [ 0.01327458]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.00076992], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]\n",
      " [-0.1284754]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.5844373  -1.1685977   0.24551131 -0.1284754 ]\n",
      " [-1.1719993  -1.4900941   0.90811324 -0.1284754 ]\n",
      " [-0.3498045  -1.3396003   0.5051281  -0.1284754 ]\n",
      " [-0.28296348 -1.3273659   0.4782912  -0.1284754 ]\n",
      " [ 1.5887318  -0.98477274  0.13118324 -0.1284754 ]\n",
      " [-0.6783197  -1.3997315   0.6577065  -0.1284754 ]\n",
      " [ 0.21988223 -1.2353256   0.3207565  -0.1284754 ]\n",
      " [-0.2344793  -1.3184913   0.45975274 -0.1284754 ]\n",
      " [ 0.27785835 -1.2247137   0.30696288 -0.1284754 ]\n",
      " [ 0.24969608 -1.2298684   0.313567   -0.1284754 ]\n",
      " [ 0.12470661 -1.2527463   0.3451398  -0.1284754 ]\n",
      " [-1.4513341  -1.5412232   0.9919848  -0.1284754 ]\n",
      " [ 0.6939638  -1.1485502   0.22753865 -0.1284754 ]\n",
      " [-0.6969245  -1.4031368   0.66722894 -0.1284754 ]\n",
      " [ 1.758624   -0.95367587  0.11966632 -0.1284754 ]\n",
      " [-0.03485066 -1.2819515   0.3913495  -0.1284754 ]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:   0.584  action:  -1.169  reward:   0.246 c_value   0.305 critic_dvda  -0.128 cl   0.004\n",
      "d:  state:  -1.172  action:  -1.490  reward:   0.908 c_value   0.742 critic_dvda  -0.128 cl   0.028\n",
      "d:  state:  -0.350  action:  -1.340  reward:   0.505 c_value   0.538 critic_dvda  -0.128 cl   0.001\n",
      "d:  state:  -0.283  action:  -1.327  reward:   0.478 c_value   0.521 critic_dvda  -0.128 cl   0.002\n",
      "d:  state:   1.589  action:  -0.985  reward:   0.131 c_value   0.056 critic_dvda  -0.128 cl   0.006\n",
      "d:  state:  -0.678  action:  -1.400  reward:   0.658 c_value   0.619 critic_dvda  -0.128 cl   0.001\n",
      "d:  state:   0.220  action:  -1.235  reward:   0.321 c_value   0.396 critic_dvda  -0.128 cl   0.006\n",
      "d:  state:  -0.234  action:  -1.318  reward:   0.460 c_value   0.509 critic_dvda  -0.128 cl   0.002\n",
      "d:  state:   0.278  action:  -1.225  reward:   0.307 c_value   0.382 critic_dvda  -0.128 cl   0.006\n",
      "d:  state:   0.250  action:  -1.230  reward:   0.314 c_value   0.389 critic_dvda  -0.128 cl   0.006\n",
      "d:  state:   0.125  action:  -1.253  reward:   0.345 c_value   0.420 critic_dvda  -0.128 cl   0.006\n",
      "d:  state:  -1.451  action:  -1.541  reward:   0.992 c_value   0.812 critic_dvda  -0.128 cl   0.033\n",
      "d:  state:   0.694  action:  -1.149  reward:   0.228 c_value   0.278 critic_dvda  -0.128 cl   0.003\n",
      "d:  state:  -0.697  action:  -1.403  reward:   0.667 c_value   0.624 critic_dvda  -0.128 cl   0.002\n",
      "d:  state:   1.759  action:  -0.954  reward:   0.120 c_value   0.014 critic_dvda  -0.128 cl   0.011\n",
      "d:  state:  -0.035  action:  -1.282  reward:   0.391 c_value   0.459 critic_dvda  -0.128 cl   0.005\n",
      "1500 0.48 0.01\n",
      "global_step  1600\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.04821512 -0.06743947]\n",
      " [ 0.04764227 -0.06663819]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.03017329  0.04220399], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.07014293]\n",
      " [ 0.00918736]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.06020207], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]\n",
      " [-0.11611164]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.1745107  -1.1157832   0.16011666 -0.11611164]\n",
      " [-0.97059125 -1.4914104   0.7866257  -0.11611164]\n",
      " [ 1.2412637  -1.104094    0.1538295  -0.11611164]\n",
      " [-0.88235956 -1.4759601   0.7394472  -0.11611164]\n",
      " [-0.85054564 -1.4703892   0.7224358  -0.11611164]\n",
      " [-0.5553015  -1.4186893   0.57292193 -0.11611164]\n",
      " [-0.6435087  -1.4341352   0.6153504  -0.11611164]\n",
      " [-0.69540477 -1.4432226   0.64134157 -0.11611164]\n",
      " [-1.2969104  -1.5485518   0.9404477  -0.11611164]\n",
      " [ 0.02436844 -1.3171837   0.35717317 -0.11611164]\n",
      " [-0.13401406 -1.3449179   0.40546763 -0.11611164]\n",
      " [ 1.3727372  -1.0810719   0.14242622 -0.11611164]\n",
      " [-0.2163418  -1.3593342   0.43357018 -0.11611164]\n",
      " [-1.455889   -1.5763905   0.9856872  -0.11611164]\n",
      " [-0.0879954  -1.3368596   0.3906766  -0.11611164]\n",
      " [ 0.52585876 -1.2293681   0.24504873 -0.11611164]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:   1.175  action:  -1.116  reward:   0.160 c_value   0.152 critic_dvda  -0.116 cl   0.000\n",
      "d:  state:  -0.971  action:  -1.491  reward:   0.787 c_value   0.671 critic_dvda  -0.116 cl   0.013\n",
      "d:  state:   1.241  action:  -1.104  reward:   0.154 c_value   0.136 critic_dvda  -0.116 cl   0.000\n",
      "d:  state:  -0.882  action:  -1.476  reward:   0.739 c_value   0.650 critic_dvda  -0.116 cl   0.008\n",
      "d:  state:  -0.851  action:  -1.470  reward:   0.722 c_value   0.642 critic_dvda  -0.116 cl   0.006\n",
      "d:  state:  -0.555  action:  -1.419  reward:   0.573 c_value   0.570 critic_dvda  -0.116 cl   0.000\n",
      "d:  state:  -0.644  action:  -1.434  reward:   0.615 c_value   0.592 critic_dvda  -0.116 cl   0.001\n",
      "d:  state:  -0.695  action:  -1.443  reward:   0.641 c_value   0.604 critic_dvda  -0.116 cl   0.001\n",
      "d:  state:  -1.297  action:  -1.549  reward:   0.940 c_value   0.750 critic_dvda  -0.116 cl   0.036\n",
      "d:  state:   0.024  action:  -1.317  reward:   0.357 c_value   0.430 critic_dvda  -0.116 cl   0.005\n",
      "d:  state:  -0.134  action:  -1.345  reward:   0.405 c_value   0.468 critic_dvda  -0.116 cl   0.004\n",
      "d:  state:   1.373  action:  -1.081  reward:   0.142 c_value   0.104 critic_dvda  -0.116 cl   0.001\n",
      "d:  state:  -0.216  action:  -1.359  reward:   0.434 c_value   0.488 critic_dvda  -0.116 cl   0.003\n",
      "d:  state:  -1.456  action:  -1.576  reward:   0.986 c_value   0.788 critic_dvda  -0.116 cl   0.039\n",
      "d:  state:  -0.088  action:  -1.337  reward:   0.391 c_value   0.457 critic_dvda  -0.116 cl   0.004\n",
      "d:  state:   0.526  action:  -1.229  reward:   0.245 c_value   0.309 critic_dvda  -0.116 cl   0.004\n",
      "1600 0.45 0.02\n",
      "global_step  1700\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.0329421  -0.04670946]\n",
      " [-0.03335419  0.04729377]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.02848777  0.04039354], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.07036143]\n",
      " [-0.04855269]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.05719469], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]\n",
      " [-0.10947204]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.21058896 -1.4020816   0.4132821  -0.10947204]\n",
      " [-0.13683733 -1.3889629   0.38943568 -0.10947204]\n",
      " [ 1.8701138  -1.0319719   0.10613343 -0.10947204]\n",
      " [-0.911776   -1.5268068   0.725551   -0.10947204]\n",
      " [-0.55020237 -1.462491    0.5457709  -0.10947204]\n",
      " [-0.41063163 -1.4376646   0.48666614 -0.10947204]\n",
      " [-1.190907   -1.5764579   0.87058765 -0.10947204]\n",
      " [-1.0566282  -1.5525727   0.80259335 -0.10947204]\n",
      " [-0.2502119  -1.4091296   0.42678684 -0.10947204]\n",
      " [-0.7894955  -1.5050559   0.661364   -0.10947204]\n",
      " [-0.14827643 -1.3909976   0.39302677 -0.10947204]\n",
      " [ 1.8232657  -1.0403051   0.10869522 -0.10947204]\n",
      " [ 1.690295   -1.0639576   0.11646993 -0.10947204]\n",
      " [ 1.6754812  -1.0665926   0.1173851  -0.10947204]\n",
      " [ 0.77355814 -1.2270242   0.19990687 -0.10947204]\n",
      " [ 1.1162175  -1.1660728   0.16106042 -0.10947204]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.211  action:  -1.402  reward:   0.413 c_value   0.483 critic_dvda  -0.109 cl   0.005\n",
      "d:  state:  -0.137  action:  -1.389  reward:   0.389 c_value   0.464 critic_dvda  -0.109 cl   0.006\n",
      "d:  state:   1.870  action:  -1.032  reward:   0.106 c_value  -0.034 critic_dvda  -0.109 cl   0.020\n",
      "d:  state:  -0.912  action:  -1.527  reward:   0.726 c_value   0.657 critic_dvda  -0.109 cl   0.005\n",
      "d:  state:  -0.550  action:  -1.462  reward:   0.546 c_value   0.567 critic_dvda  -0.109 cl   0.000\n",
      "d:  state:  -0.411  action:  -1.438  reward:   0.487 c_value   0.532 critic_dvda  -0.109 cl   0.002\n",
      "d:  state:  -1.191  action:  -1.576  reward:   0.871 c_value   0.726 critic_dvda  -0.109 cl   0.021\n",
      "d:  state:  -1.057  action:  -1.553  reward:   0.803 c_value   0.693 critic_dvda  -0.109 cl   0.012\n",
      "d:  state:  -0.250  action:  -1.409  reward:   0.427 c_value   0.492 critic_dvda  -0.109 cl   0.004\n",
      "d:  state:  -0.789  action:  -1.505  reward:   0.661 c_value   0.626 critic_dvda  -0.109 cl   0.001\n",
      "d:  state:  -0.148  action:  -1.391  reward:   0.393 c_value   0.467 critic_dvda  -0.109 cl   0.005\n",
      "d:  state:   1.823  action:  -1.040  reward:   0.109 c_value  -0.022 critic_dvda  -0.109 cl   0.017\n",
      "d:  state:   1.690  action:  -1.064  reward:   0.116 c_value   0.011 critic_dvda  -0.109 cl   0.011\n",
      "d:  state:   1.675  action:  -1.067  reward:   0.117 c_value   0.014 critic_dvda  -0.109 cl   0.011\n",
      "d:  state:   0.774  action:  -1.227  reward:   0.200 c_value   0.238 critic_dvda  -0.109 cl   0.001\n",
      "d:  state:   1.116  action:  -1.166  reward:   0.161 c_value   0.153 critic_dvda  -0.109 cl   0.000\n",
      "1700 0.45 0.01\n",
      "global_step  1800\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.06424689  0.0916286 ]\n",
      " [-0.0211274   0.0301318 ]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([ 0.04293077 -0.06122766], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[0.10764176]\n",
      " [0.0250953 ]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([0.08714622], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]\n",
      " [-0.09752116]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.2574572  -1.3592751   0.27671525 -0.09752116]\n",
      " [-0.37625152 -1.4753062   0.45291474 -0.09752116]\n",
      " [ 1.1022812  -1.2045891   0.15818651 -0.09752116]\n",
      " [ 0.95101154 -1.2322863   0.17340672 -0.09752116]\n",
      " [-2.4098442  -1.8476539   0.75984496 -0.09752116]\n",
      " [ 0.8683675  -1.2474184   0.18259671 -0.09752116]\n",
      " [ 0.24957797 -1.3607178   0.2783149  -0.09752116]\n",
      " [-0.7335228  -1.540722    0.60548425 -0.09752116]\n",
      " [-1.8815944  -1.7509322   0.983214   -0.09752116]\n",
      " [ 0.767989   -1.2657975   0.19469315 -0.09752116]\n",
      " [ 0.72403854 -1.2738447   0.20033905 -0.09752116]\n",
      " [-0.31642744 -1.4643525   0.4314563  -0.09752116]\n",
      " [-0.03327481 -1.4125077   0.3445563  -0.09752116]\n",
      " [ 0.62704325 -1.2916044   0.21362005 -0.09752116]\n",
      " [ 0.7180512  -1.274941    0.20112559 -0.09752116]\n",
      " [-2.20568    -1.8102717   0.86479187 -0.09752116]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:   0.257  action:  -1.359  reward:   0.277 c_value   0.352 critic_dvda  -0.098 cl   0.006\n",
      "d:  state:  -0.376  action:  -1.475  reward:   0.453 c_value   0.503 critic_dvda  -0.098 cl   0.003\n",
      "d:  state:   1.102  action:  -1.205  reward:   0.158 c_value   0.150 critic_dvda  -0.098 cl   0.000\n",
      "d:  state:   0.951  action:  -1.232  reward:   0.173 c_value   0.186 critic_dvda  -0.098 cl   0.000\n",
      "d:  state:  -2.410  action:  -1.848  reward:   0.760 c_value   0.989 critic_dvda  -0.098 cl   0.053\n",
      "d:  state:   0.868  action:  -1.247  reward:   0.183 c_value   0.206 critic_dvda  -0.098 cl   0.001\n",
      "d:  state:   0.250  action:  -1.361  reward:   0.278 c_value   0.354 critic_dvda  -0.098 cl   0.006\n",
      "d:  state:  -0.734  action:  -1.541  reward:   0.605 c_value   0.589 critic_dvda  -0.098 cl   0.000\n",
      "d:  state:  -1.882  action:  -1.751  reward:   0.983 c_value   0.863 critic_dvda  -0.098 cl   0.014\n",
      "d:  state:   0.768  action:  -1.266  reward:   0.195 c_value   0.230 critic_dvda  -0.098 cl   0.001\n",
      "d:  state:   0.724  action:  -1.274  reward:   0.200 c_value   0.240 critic_dvda  -0.098 cl   0.002\n",
      "d:  state:  -0.316  action:  -1.464  reward:   0.431 c_value   0.489 critic_dvda  -0.098 cl   0.003\n",
      "d:  state:  -0.033  action:  -1.413  reward:   0.345 c_value   0.421 critic_dvda  -0.098 cl   0.006\n",
      "d:  state:   0.627  action:  -1.292  reward:   0.214 c_value   0.264 critic_dvda  -0.098 cl   0.002\n",
      "d:  state:   0.718  action:  -1.275  reward:   0.201 c_value   0.242 critic_dvda  -0.098 cl   0.002\n",
      "d:  state:  -2.206  action:  -1.810  reward:   0.865 c_value   0.940 critic_dvda  -0.098 cl   0.006\n",
      "1800 0.44 0.01\n",
      "global_step  1900\n",
      "critic_grads--------------------------------------------------------------------------------:\n",
      "g\n",
      "tf.Tensor(\n",
      "[[ 0.04024119 -0.05729654]\n",
      " [ 0.05830515 -0.08301653]], shape=(2, 2), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.02034398  0.02896634], shape=(2,), dtype=float32)\n",
      "g\n",
      "tf.Tensor(\n",
      "[[-0.0507282 ]\n",
      " [ 0.02057525]], shape=(2, 1), dtype=float32)\n",
      "g\n",
      "tf.Tensor([-0.0414739], shape=(1,), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "tf.Tensor(\n",
      "[[-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]\n",
      " [-0.09290877]], shape=(16, 1), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[-0.54863    -1.5476831   0.5004737  -0.09290877]\n",
      " [ 0.6095427  -1.3325973   0.20955978 -0.09290877]\n",
      " [-0.10184983 -1.464711    0.34996969 -0.09290877]\n",
      " [ 0.06212073 -1.4342598   0.30872256 -0.09290877]\n",
      " [-1.5524614  -1.734106    0.9680591  -0.09290877]\n",
      " [-1.2231233  -1.6729442   0.8317123  -0.09290877]\n",
      " [-1.220973   -1.6725448   0.83062196 -0.09290877]\n",
      " [-0.41576385 -1.5230083   0.44923827 -0.09290877]\n",
      " [ 1.0243186  -1.2555686   0.16134535 -0.09290877]\n",
      " [ 1.6248499  -1.1440432   0.11538324 -0.09290877]\n",
      " [-1.1149216  -1.6528499   0.7755744  -0.09290877]\n",
      " [ 1.1169773  -1.2383609   0.1527273  -0.09290877]\n",
      " [-1.5307198  -1.7300683   0.96177906 -0.09290877]\n",
      " [-0.25170642 -1.492541    0.39375192 -0.09290877]\n",
      " [ 0.52256316 -1.3487504   0.22213265 -0.09290877]\n",
      " [-0.14892843 -1.473454    0.36305967 -0.09290877]], shape=(16, 4), dtype=float32)\n",
      "critic_dvda::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n",
      "(16, 1)\n",
      "d:  state:  -0.549  action:  -1.548  reward:   0.500 c_value   0.535 critic_dvda  -0.093 cl   0.001\n",
      "d:  state:   0.610  action:  -1.333  reward:   0.210 c_value   0.268 critic_dvda  -0.093 cl   0.003\n",
      "d:  state:  -0.102  action:  -1.465  reward:   0.350 c_value   0.432 critic_dvda  -0.093 cl   0.007\n",
      "d:  state:   0.062  action:  -1.434  reward:   0.309 c_value   0.394 critic_dvda  -0.093 cl   0.007\n",
      "d:  state:  -1.552  action:  -1.734  reward:   0.968 c_value   0.767 critic_dvda  -0.093 cl   0.040\n",
      "d:  state:  -1.223  action:  -1.673  reward:   0.832 c_value   0.691 critic_dvda  -0.093 cl   0.020\n",
      "d:  state:  -1.221  action:  -1.673  reward:   0.831 c_value   0.691 critic_dvda  -0.093 cl   0.020\n",
      "d:  state:  -0.416  action:  -1.523  reward:   0.449 c_value   0.504 critic_dvda  -0.093 cl   0.003\n",
      "d:  state:   1.024  action:  -1.256  reward:   0.161 c_value   0.172 critic_dvda  -0.093 cl   0.000\n",
      "d:  state:   1.625  action:  -1.144  reward:   0.115 c_value   0.033 critic_dvda  -0.093 cl   0.007\n",
      "d:  state:  -1.115  action:  -1.653  reward:   0.776 c_value   0.666 critic_dvda  -0.093 cl   0.012\n",
      "d:  state:   1.117  action:  -1.238  reward:   0.153 c_value   0.150 critic_dvda  -0.093 cl   0.000\n",
      "d:  state:  -1.531  action:  -1.730  reward:   0.962 c_value   0.762 critic_dvda  -0.093 cl   0.040\n",
      "d:  state:  -0.252  action:  -1.493  reward:   0.394 c_value   0.467 critic_dvda  -0.093 cl   0.005\n",
      "d:  state:   0.523  action:  -1.349  reward:   0.222 c_value   0.288 critic_dvda  -0.093 cl   0.004\n",
      "d:  state:  -0.149  action:  -1.473  reward:   0.363 c_value   0.443 critic_dvda  -0.093 cl   0.006\n",
      "1900 0.44 0.01\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def r(net_out: float, state: float) -> float:\n",
    "    delta = net_out - state\n",
    "    probability = 1 / (1 + delta * delta)\n",
    "    return probability\n",
    "    # if random.uniform(0, 1) < probability:\n",
    "    #     return 1\n",
    "    # else:\n",
    "    #     return 0\n",
    "\n",
    "def create_block(model: ZModel):\n",
    "    block_size = 16\n",
    "    state = tf.random.normal(shape=(block_size, 1))\n",
    "    actions= model.actor(state)\n",
    "    rewards = np.array([r(state[i,0],actions[i, 0] ) for i in range(block_size)]).reshape((block_size,1))\n",
    "    return state, actions, rewards\n",
    "\n",
    "\n",
    "class RingSum:\n",
    "    def __init__(self):\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "        self.max = 10\n",
    "        self.values = []\n",
    "        self.index = 0\n",
    "    def add(self, x):\n",
    "        if len(self.values) < self.max:\n",
    "            self.values.append(x)\n",
    "            self.sum += x\n",
    "        else:\n",
    "            xx = self.values[self.index]\n",
    "            self.values[self.index] = x\n",
    "            self.sum -= xx\n",
    "            self.sum += x\n",
    "            self.index = (self.index + 1) % self.max\n",
    "        return self.sum\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / len(self.values)\n",
    "\n",
    "def train():\n",
    "    model = ZModel(1)\n",
    "    rewards = RingSum()\n",
    "    losses = RingSum()\n",
    "    for i in range(2000):\n",
    "        state, action, reward = create_block(model)\n",
    "        loss = model.update(state, reward)\n",
    "        rewards.add(np.mean(reward))\n",
    "        losses.add(np.mean(loss))\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            print(\"%4d %.2f %.2f\" % (i, rewards.mean(), losses.mean()))\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}